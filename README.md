# OFormers

Transformers unable to scale on long sequences.
This repo is dedicated to explore alternatives to softmax attention and find **O**ptimal trans**Former** achitectures for processing long sequeneces.